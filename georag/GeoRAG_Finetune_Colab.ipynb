{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a1b186",
   "metadata": {},
   "source": [
    "# GeoRAG — Fine-tune Mistral-7B on Planetary QA Data\n",
    "\n",
    "This notebook runs the full pipeline on Google Colab (T4 or A100):\n",
    "1. Install deps\n",
    "2. Upload your QA dataset\n",
    "3. Fine-tune with QLoRA\n",
    "4. Test the model\n",
    "5. Download the LoRA adapter weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f13102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what GPU we got\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa159bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    torch>=2.1.0 \\\n",
    "    transformers>=4.38.0 \\\n",
    "    peft>=0.9.0 \\\n",
    "    bitsandbytes>=0.43.0 \\\n",
    "    accelerate>=0.27.0 \\\n",
    "    datasets>=2.18.0 \\\n",
    "    sentence-transformers>=2.5.0 \\\n",
    "    sentencepiece>=0.1.99 \\\n",
    "    wandb>=0.16.0 \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) log in to wandb — skip if you don't want experiment tracking\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d32521",
   "metadata": {},
   "source": [
    "## Upload QA dataset\n",
    "\n",
    "Upload `qa_train.jsonl` and `qa_test.jsonl` from `georag/outputs/`.\n",
    "Or just run the cell below to upload via Colab's file picker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c904b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs(\"georag_data\", exist_ok=True)\n",
    "\n",
    "print(\"Upload qa_train.jsonl and qa_test.jsonl\")\n",
    "uploaded = files.upload()\n",
    "for name, data in uploaded.items():\n",
    "    with open(f\"georag_data/{name}\", \"wb\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"  saved {name} ({len(data)} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3164c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l georag_data/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ebf32",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# model\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# lora\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "]\n",
    "\n",
    "# training\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM = 8           # effective batch = 32\n",
    "LR = 2e-4\n",
    "WARMUP_RATIO = 0.06\n",
    "MAX_SEQ_LEN = 1024\n",
    "PATIENCE = 3\n",
    "\n",
    "# paths\n",
    "TRAIN_FILE = Path(\"georag_data/qa_train.jsonl\")\n",
    "TEST_FILE = Path(\"georag_data/qa_test.jsonl\")\n",
    "OUTPUT_DIR = Path(\"georag_lora_weights\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# wandb (set to False to skip)\n",
    "USE_WANDB = False\n",
    "WANDB_PROJECT = \"georag\"\n",
    "WANDB_RUN = \"mistral-7b-lora-r16-colab\"\n",
    "\n",
    "print(\"config loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd7db6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are GeoRAG, an expert assistant for NASA planetary science. \"\n",
    "    \"Answer questions about surface features on the Moon, Mars, Mercury, \"\n",
    "    \"and other celestial bodies using precise nomenclature data.\"\n",
    ")\n",
    "\n",
    "def format_prompt(question, context=\"\"):\n",
    "    parts = [f\"[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n\"]\n",
    "    if context:\n",
    "        parts.append(f\"Context:\\n{context}\\n\\n\")\n",
    "    parts.append(f\"{question} [/INST]\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len=MAX_SEQ_LEN):\n",
    "        self.samples = [json.loads(line) for line in open(path)]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        prompt = format_prompt(item[\"question\"])\n",
    "        full = f\"{prompt} {item['answer']}{self.tokenizer.eos_token}\"\n",
    "\n",
    "        enc = self.tokenizer(full, max_length=self.max_len,\n",
    "                             padding=\"max_length\", truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attn_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # only compute loss on the answer tokens\n",
    "        prompt_len = self.tokenizer(prompt, max_length=self.max_len,\n",
    "                                    truncation=True, return_tensors=\"pt\"\n",
    "                                    )[\"input_ids\"].shape[1]\n",
    "        labels = input_ids.clone()\n",
    "        labels[:prompt_len] = -100\n",
    "        labels[attn_mask == 0] = -100\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn_mask,\n",
    "                \"labels\": labels}\n",
    "\n",
    "print(\"dataset class ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c532fac",
   "metadata": {},
   "source": [
    "## Load model + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70800507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataloaders\n",
    "train_ds = QADataset(TRAIN_FILE, tokenizer)\n",
    "val_ds = QADataset(TEST_FILE, tokenizer)\n",
    "print(f\"train: {len(train_ds)}, val: {len(val_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          drop_last=True, pin_memory=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858823e",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f24535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.init(project=WANDB_PROJECT, name=WANDB_RUN, config={\n",
    "        \"model\": MODEL_NAME, \"lora_r\": LORA_R, \"lora_alpha\": LORA_ALPHA,\n",
    "        \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE,\n",
    "        \"effective_batch\": BATCH_SIZE * GRAD_ACCUM, \"lr\": LR,\n",
    "    })\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    (p for p in model.parameters() if p.requires_grad),\n",
    "    lr=LR, weight_decay=0.01,\n",
    ")\n",
    "\n",
    "total_steps = (len(train_loader) // GRAD_ACCUM) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "def eval_loss():\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            total += model(**batch).loss.item()\n",
    "            n += 1\n",
    "    return total / max(n, 1)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"epoch {epoch}/{EPOCHS}\")\n",
    "    for step, batch in enumerate(pbar, 1):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        loss = out.loss / GRAD_ACCUM\n",
    "        loss.backward()\n",
    "        epoch_loss += out.loss.item()\n",
    "\n",
    "        if step % GRAD_ACCUM == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{out.loss.item():.4f}\",\n",
    "                             lr=f\"{scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "            if USE_WANDB:\n",
    "                wandb.log({\"train/loss\": out.loss.item(),\n",
    "                           \"train/lr\": scheduler.get_last_lr()[0],\n",
    "                           \"global_step\": global_step})\n",
    "\n",
    "    avg_train = epoch_loss / len(train_loader)\n",
    "    vloss = eval_loss()\n",
    "    print(f\"epoch {epoch}/{EPOCHS}  train_loss={avg_train:.4f}  val_loss={vloss:.4f}\")\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.log({\"epoch\": epoch, \"train/epoch_loss\": avg_train, \"val/loss\": vloss})\n",
    "\n",
    "    # early stopping\n",
    "    if vloss < best_val_loss:\n",
    "        best_val_loss = vloss\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(str(OUTPUT_DIR))\n",
    "        tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "        print(f\"  saved best model -> {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  no improvement ({patience_counter}/{PATIENCE})\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"  early stopping.\")\n",
    "            break\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "\n",
    "print(f\"\\ndone. best val loss = {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e5547",
   "metadata": {},
   "source": [
    "## Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the best checkpoint\n",
    "from peft import PeftModel\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_questions = [\n",
    "    \"What type of feature is Tycho?\",\n",
    "    \"Where is Olympus Mons located?\",\n",
    "    \"Is Caloris Planitia on Mars?\",\n",
    "    \"Name some valleys on Mars.\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    prompt = format_prompt(q)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
    "                       max_length=MAX_SEQ_LEN).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=200,\n",
    "                             temperature=0.3, top_p=0.9,\n",
    "                             repetition_penalty=1.15, do_sample=True)\n",
    "    answer = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                              skip_special_tokens=True)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer.strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb353886",
   "metadata": {},
   "source": [
    "## Download weights\n",
    "\n",
    "Download the LoRA adapter to use locally with the RAG pipeline and eval harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip and download\n",
    "!cd georag_lora_weights && zip -r /content/georag_lora_weights.zip .\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"/content/georag_lora_weights.zip\")\n",
    "print(\"download started — put the zip contents into georag/outputs/georag-mistral-lora/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
